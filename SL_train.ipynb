{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098e2f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_5subj.npz: (1005, 22, 1000)\n",
      "Loaded val_id_5subj.npz: (215, 22, 1000)\n",
      "\n",
      " Train HybridNorm EEGNet\n",
      "Epoch | TrainLoss | ValLoss | TrainAcc | ValAcc | LR\n",
      "001 | 1.4040 | 1.3886 | 24.38% | 23.72% | 3.0e-04\n",
      "002 | 1.3896 | 1.3849 | 25.27% | 21.40% | 3.0e-04\n",
      "003 | 1.3868 | 1.3814 | 26.87% | 27.91% | 3.0e-04\n",
      "004 | 1.3884 | 1.3806 | 27.06% | 26.05% | 3.0e-04\n",
      "005 | 1.3799 | 1.3770 | 27.96% | 33.02% | 3.0e-04\n",
      "006 | 1.3787 | 1.3738 | 27.76% | 33.49% | 3.0e-04\n",
      "007 | 1.3810 | 1.3705 | 28.76% | 28.37% | 3.0e-04\n",
      "008 | 1.3721 | 1.3641 | 28.96% | 32.09% | 3.0e-04\n",
      "009 | 1.3663 | 1.3568 | 30.55% | 34.42% | 3.0e-04\n",
      "010 | 1.3536 | 1.3426 | 31.74% | 32.56% | 3.0e-04\n",
      "011 | 1.3376 | 1.3284 | 36.02% | 36.28% | 3.0e-04\n",
      "012 | 1.3226 | 1.3175 | 37.31% | 38.14% | 3.0e-04\n",
      "013 | 1.2971 | 1.3096 | 38.31% | 38.60% | 3.0e-04\n",
      "014 | 1.2982 | 1.3028 | 37.31% | 39.53% | 3.0e-04\n",
      "015 | 1.2795 | 1.2991 | 39.10% | 40.47% | 3.0e-04\n",
      "016 | 1.2703 | 1.2994 | 40.10% | 39.07% | 3.0e-04\n",
      "017 | 1.2482 | 1.2883 | 42.09% | 39.53% | 3.0e-04\n",
      "018 | 1.2579 | 1.2777 | 40.80% | 38.14% | 3.0e-04\n",
      "019 | 1.2447 | 1.2758 | 40.50% | 38.14% | 3.0e-04\n",
      "020 | 1.2158 | 1.2623 | 43.58% | 40.93% | 3.0e-04\n",
      "021 | 1.2250 | 1.2522 | 42.49% | 40.00% | 3.0e-04\n",
      "022 | 1.2156 | 1.2379 | 42.89% | 45.12% | 3.0e-04\n",
      "023 | 1.2119 | 1.2375 | 43.88% | 41.40% | 3.0e-04\n",
      "024 | 1.1914 | 1.2264 | 44.88% | 45.12% | 3.0e-04\n",
      "025 | 1.1825 | 1.2256 | 43.98% | 43.72% | 3.0e-04\n",
      "026 | 1.1835 | 1.2137 | 43.58% | 44.65% | 3.0e-04\n",
      "027 | 1.1533 | 1.2310 | 49.35% | 43.72% | 3.0e-04\n",
      "028 | 1.1217 | 1.2225 | 50.35% | 45.12% | 3.0e-04\n",
      "029 | 1.1337 | 1.2080 | 48.26% | 44.19% | 3.0e-04\n",
      "030 | 1.1259 | 1.2146 | 49.55% | 45.12% | 3.0e-04\n",
      "031 | 1.1500 | 1.2078 | 47.86% | 46.05% | 3.0e-04\n",
      "032 | 1.0993 | 1.2053 | 50.05% | 47.44% | 3.0e-04\n",
      "033 | 1.1152 | 1.2056 | 51.64% | 47.91% | 3.0e-04\n",
      "034 | 1.1027 | 1.2055 | 50.85% | 46.51% | 3.0e-04\n",
      "035 | 1.0748 | 1.2104 | 51.24% | 46.05% | 3.0e-04\n",
      "036 | 1.0692 | 1.2072 | 52.94% | 46.98% | 3.0e-04\n",
      "037 | 1.0823 | 1.2084 | 49.55% | 47.91% | 3.0e-04\n",
      "038 | 1.0954 | 1.2041 | 50.75% | 45.12% | 3.0e-04\n",
      "039 | 1.0500 | 1.1949 | 53.23% | 47.44% | 3.0e-04\n",
      "040 | 1.0623 | 1.1891 | 51.54% | 46.98% | 3.0e-04\n",
      "041 | 1.0258 | 1.1834 | 54.63% | 47.91% | 3.0e-04\n",
      "042 | 1.0677 | 1.1751 | 51.54% | 47.91% | 3.0e-04\n",
      "043 | 1.0265 | 1.1680 | 54.23% | 48.37% | 3.0e-04\n",
      "044 | 1.0364 | 1.1637 | 53.43% | 49.30% | 3.0e-04\n",
      "045 | 1.0205 | 1.1527 | 56.02% | 50.23% | 3.0e-04\n",
      "046 | 1.0125 | 1.1655 | 56.12% | 46.98% | 3.0e-04\n",
      "047 | 1.0431 | 1.1450 | 54.73% | 50.23% | 3.0e-04\n",
      "048 | 1.0060 | 1.1403 | 55.42% | 52.09% | 3.0e-04\n",
      "049 | 1.0055 | 1.1417 | 56.22% | 49.30% | 3.0e-04\n",
      "050 | 1.0081 | 1.1497 | 55.52% | 51.63% | 3.0e-04\n",
      "051 | 1.0021 | 1.1533 | 55.22% | 50.23% | 3.0e-04\n",
      "052 | 1.0064 | 1.1551 | 55.62% | 48.37% | 3.0e-04\n",
      "053 | 0.9958 | 1.1336 | 55.82% | 51.16% | 3.0e-04\n",
      "054 | 0.9932 | 1.1247 | 54.43% | 53.49% | 3.0e-04\n",
      "055 | 0.9859 | 1.1140 | 55.42% | 51.63% | 3.0e-04\n",
      "056 | 0.9632 | 1.1282 | 57.01% | 52.56% | 3.0e-04\n",
      "057 | 0.9675 | 1.1273 | 57.61% | 51.63% | 3.0e-04\n",
      "058 | 0.9767 | 1.1162 | 57.91% | 52.56% | 3.0e-04\n",
      "059 | 0.9702 | 1.1077 | 54.53% | 53.49% | 3.0e-04\n",
      "060 | 0.9682 | 1.1149 | 57.71% | 53.02% | 3.0e-04\n",
      "061 | 0.9569 | 1.1094 | 58.41% | 52.09% | 3.0e-04\n",
      "062 | 0.9779 | 1.0991 | 56.02% | 54.42% | 3.0e-04\n",
      "063 | 0.9449 | 1.1088 | 60.60% | 53.49% | 3.0e-04\n",
      "064 | 0.9306 | 1.1176 | 60.10% | 51.63% | 3.0e-04\n",
      "065 | 0.9268 | 1.1182 | 60.20% | 53.02% | 3.0e-04\n",
      "066 | 0.9405 | 1.1282 | 57.91% | 51.63% | 3.0e-04\n",
      "067 | 0.9477 | 1.1164 | 57.61% | 52.56% | 3.0e-04\n",
      "068 | 0.9363 | 1.1202 | 59.60% | 52.09% | 3.0e-04\n",
      "069 | 0.9237 | 1.0927 | 59.40% | 52.56% | 3.0e-04\n",
      "070 | 0.9126 | 1.0969 | 61.99% | 53.49% | 3.0e-04\n",
      "071 | 0.9190 | 1.0989 | 60.30% | 54.42% | 3.0e-04\n",
      "072 | 0.9274 | 1.1036 | 60.30% | 53.95% | 3.0e-04\n",
      "073 | 0.8916 | 1.0987 | 61.89% | 54.42% | 3.0e-04\n",
      "074 | 0.9411 | 1.1064 | 58.81% | 55.35% | 3.0e-04\n",
      "075 | 0.9029 | 1.0939 | 61.29% | 56.28% | 3.0e-04\n",
      "076 | 0.8930 | 1.0993 | 59.40% | 53.95% | 3.0e-04\n",
      "077 | 0.8895 | 1.1054 | 61.89% | 54.42% | 3.0e-04\n",
      "078 | 0.8829 | 1.1123 | 61.29% | 51.63% | 3.0e-04\n",
      "079 | 0.8847 | 1.1034 | 61.79% | 52.56% | 3.0e-04\n",
      "080 | 0.8861 | 1.0879 | 61.89% | 55.35% | 3.0e-04\n",
      "081 | 0.8934 | 1.0952 | 62.09% | 56.28% | 3.0e-04\n",
      "082 | 0.8903 | 1.0992 | 62.89% | 53.49% | 3.0e-04\n",
      "083 | 0.8736 | 1.0905 | 61.19% | 56.28% | 3.0e-04\n",
      "084 | 0.8743 | 1.1106 | 60.70% | 53.95% | 3.0e-04\n",
      "085 | 0.8564 | 1.1000 | 64.28% | 54.88% | 3.0e-04\n",
      "086 | 0.8921 | 1.0929 | 61.00% | 53.02% | 3.0e-04\n",
      "087 | 0.8827 | 1.1018 | 61.49% | 51.63% | 3.0e-04\n",
      "088 | 0.9063 | 1.0880 | 60.80% | 52.56% | 3.0e-04\n",
      "089 | 0.8792 | 1.0931 | 61.19% | 54.42% | 3.0e-04\n",
      "090 | 0.8668 | 1.0888 | 61.49% | 55.35% | 3.0e-04\n",
      "091 | 0.8446 | 1.0854 | 64.18% | 51.63% | 3.0e-04\n",
      "092 | 0.8401 | 1.1003 | 63.88% | 53.02% | 3.0e-04\n",
      "093 | 0.8532 | 1.0976 | 61.59% | 52.56% | 3.0e-04\n",
      "094 | 0.8844 | 1.0769 | 62.39% | 55.35% | 3.0e-04\n",
      "095 | 0.8707 | 1.0830 | 63.28% | 56.28% | 3.0e-04\n",
      "096 | 0.8454 | 1.0792 | 63.68% | 57.21% | 3.0e-04\n",
      "097 | 0.8269 | 1.0878 | 65.47% | 56.74% | 3.0e-04\n",
      "098 | 0.8503 | 1.1029 | 62.19% | 51.63% | 3.0e-04\n",
      "099 | 0.8517 | 1.0706 | 63.18% | 56.74% | 3.0e-04\n",
      "100 | 0.8128 | 1.0709 | 65.07% | 54.42% | 3.0e-04\n",
      "101 | 0.8387 | 1.0938 | 63.98% | 53.49% | 3.0e-04\n",
      "102 | 0.8506 | 1.0898 | 62.49% | 55.35% | 3.0e-04\n",
      "103 | 0.8347 | 1.0695 | 65.17% | 55.35% | 3.0e-04\n",
      "104 | 0.8270 | 1.0779 | 65.17% | 54.42% | 3.0e-04\n",
      "105 | 0.8369 | 1.0962 | 65.47% | 54.88% | 3.0e-04\n",
      "106 | 0.8249 | 1.0745 | 64.58% | 54.88% | 3.0e-04\n",
      "107 | 0.8343 | 1.0800 | 64.68% | 55.81% | 3.0e-04\n",
      "108 | 0.8140 | 1.0703 | 64.68% | 53.95% | 3.0e-04\n",
      "109 | 0.7672 | 1.0935 | 69.55% | 53.02% | 3.0e-04\n",
      "110 | 0.8229 | 1.0939 | 65.37% | 53.02% | 3.0e-04\n",
      "111 | 0.8295 | 1.0742 | 65.97% | 56.74% | 3.0e-04\n",
      "112 | 0.8282 | 1.0739 | 64.48% | 56.74% | 3.0e-04\n",
      "113 | 0.8201 | 1.0617 | 63.58% | 54.88% | 3.0e-04\n",
      "114 | 0.8117 | 1.0687 | 65.87% | 57.21% | 3.0e-04\n",
      "115 | 0.7956 | 1.0697 | 66.57% | 56.28% | 3.0e-04\n",
      "116 | 0.8190 | 1.0639 | 66.27% | 54.42% | 3.0e-04\n",
      "117 | 0.7953 | 1.0651 | 64.98% | 57.67% | 3.0e-04\n",
      "118 | 0.7856 | 1.0863 | 64.88% | 55.81% | 3.0e-04\n",
      "119 | 0.8203 | 1.0783 | 65.27% | 58.14% | 3.0e-04\n",
      "120 | 0.7821 | 1.0730 | 67.66% | 57.67% | 3.0e-04\n",
      "121 | 0.8021 | 1.0700 | 66.67% | 56.74% | 3.0e-04\n",
      "122 | 0.7989 | 1.1160 | 65.87% | 52.56% | 3.0e-04\n",
      "123 | 0.7901 | 1.0764 | 65.07% | 57.21% | 3.0e-04\n",
      "124 | 0.7544 | 1.1061 | 67.56% | 55.35% | 3.0e-04\n",
      "125 | 0.7408 | 1.1127 | 70.55% | 53.49% | 3.0e-04\n",
      "126 | 0.7737 | 1.1027 | 68.26% | 55.35% | 2.1e-04\n",
      "127 | 0.7994 | 1.0952 | 66.27% | 54.88% | 2.1e-04\n",
      "128 | 0.7383 | 1.0785 | 69.15% | 55.35% | 2.1e-04\n",
      "129 | 0.7631 | 1.0831 | 66.67% | 54.88% | 2.1e-04\n",
      "130 | 0.7596 | 1.0863 | 67.36% | 53.95% | 2.1e-04\n",
      "131 | 0.7735 | 1.0922 | 66.37% | 55.35% | 2.1e-04\n",
      "132 | 0.7554 | 1.1002 | 68.16% | 55.35% | 2.1e-04\n",
      "133 | 0.7763 | 1.0917 | 66.67% | 56.28% | 2.1e-04\n",
      "134 | 0.7631 | 1.0872 | 68.06% | 57.21% | 2.1e-04\n",
      "135 | 0.7780 | 1.0933 | 68.26% | 56.28% | 2.1e-04\n",
      "136 | 0.7401 | 1.0933 | 69.35% | 55.35% | 2.1e-04\n",
      "137 | 0.7708 | 1.1074 | 68.86% | 54.42% | 2.1e-04\n",
      "138 | 0.7427 | 1.0985 | 68.06% | 55.35% | 2.1e-04\n",
      "139 | 0.7715 | 1.0841 | 68.36% | 56.74% | 1.5e-04\n",
      "140 | 0.7604 | 1.0791 | 68.46% | 56.74% | 1.5e-04\n",
      "141 | 0.7466 | 1.0909 | 68.76% | 55.35% | 1.5e-04\n",
      "142 | 0.7438 | 1.0968 | 68.66% | 55.81% | 1.5e-04\n",
      "143 | 0.7517 | 1.0935 | 69.25% | 56.28% | 1.5e-04\n",
      "144 | 0.7410 | 1.0932 | 69.15% | 57.21% | 1.5e-04\n",
      "145 | 0.7552 | 1.0863 | 69.05% | 57.67% | 1.5e-04\n",
      "146 | 0.7159 | 1.0839 | 69.45% | 56.74% | 1.5e-04\n",
      "147 | 0.7281 | 1.0835 | 70.95% | 57.21% | 1.5e-04\n",
      "148 | 0.7496 | 1.0991 | 67.36% | 57.67% | 1.5e-04\n",
      "149 | 0.7366 | 1.1040 | 68.36% | 56.74% | 1.5e-04\n",
      "150 | 0.7346 | 1.1065 | 70.35% | 56.74% | 1.5e-04\n",
      "151 | 0.7196 | 1.1025 | 69.45% | 55.35% | 1.5e-04\n",
      "152 | 0.7325 | 1.0931 | 69.25% | 56.28% | 1.0e-04\n",
      "153 | 0.7326 | 1.0904 | 70.15% | 55.81% | 1.0e-04\n",
      "154 | 0.7487 | 1.0910 | 68.56% | 56.28% | 1.0e-04\n",
      "155 | 0.6949 | 1.1014 | 71.94% | 56.74% | 1.0e-04\n",
      "156 | 0.7363 | 1.1004 | 69.85% | 56.28% | 1.0e-04\n",
      "157 | 0.7269 | 1.0963 | 69.15% | 55.81% | 1.0e-04\n",
      "158 | 0.7383 | 1.0980 | 69.55% | 56.28% | 1.0e-04\n",
      "159 | 0.7128 | 1.0986 | 69.65% | 54.88% | 1.0e-04\n",
      " Early stopping at epoch 159\n",
      "\n",
      " Done. Best Val Acc = 58.14%  (saved: models\\pooled_eegnet_best.pth)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "import time\n",
    "\n",
    "# ========== 路径 ==========\n",
    "data_dir = \"data_processed\"\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ========== 模型结构 ==========\n",
    "class EEGNetHybridNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    HybridNorm EEGNet:\n",
    "    - 在每个卷积块中保留 BatchNorm，同时叠加 GroupNorm(num_groups=1) ≈ LayerNorm（跨通道归一）\n",
    "    - 比纯 BatchNorm 更稳地应对跨被试分布差异\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4, num_channels=22, sample_length=1000, dropout_rate=0.35):\n",
    "        super().__init__()\n",
    "        # block1: time conv -> BN -> spatial conv -> BN -> GN -> act -> pool -> dropout\n",
    "        self.temporal = nn.Conv2d(1, 16, (1, 64), padding=(0, 32), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.spatial = nn.Conv2d(16, 32, (num_channels, 1), groups=16, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.gn2 = nn.GroupNorm(1, 32)  # = LayerNorm over channels\n",
    "        self.act1 = nn.LeakyReLU(0.1)\n",
    "        self.pool1 = nn.AvgPool2d((1, 4))\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # block2: depthwise time conv -> pointwise -> BN -> GN -> act -> pool -> dropout\n",
    "        self.dw_time = nn.Conv2d(32, 32, (1, 16), groups=32, padding=(0, 8), bias=False)\n",
    "        self.pw = nn.Conv2d(32, 32, (1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.gn3 = nn.GroupNorm(1, 32)\n",
    "        self.act2 = nn.LeakyReLU(0.1)\n",
    "        self.pool2 = nn.AvgPool2d((1, 8))\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 1, num_channels, sample_length)\n",
    "            out = self._forward_features(dummy)\n",
    "            fc_in = out.view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fc_in, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.temporal(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.spatial(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.dw_time(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self._forward_features(x)\n",
    "            return x.view(x.size(0), -1)\n",
    "\n",
    "def augment_batch(x, noise_std=0.03, p_shift=0.3, max_shift=10, p_scale=0.3, scale_lo=0.9, scale_hi=1.1):\n",
    "    # 高斯噪声\n",
    "    x = x + noise_std * torch.randn_like(x)\n",
    "    # 随机时移\n",
    "    if torch.rand(1).item() < p_shift:\n",
    "        shift = int(torch.empty(1).uniform_(-max_shift, max_shift).round().item())\n",
    "        if shift != 0:\n",
    "            x = torch.roll(x, shifts=shift, dims=-1)\n",
    "    # 轻缩放\n",
    "    if torch.rand(1).item() < p_scale:\n",
    "        factor = float(torch.empty(1).uniform_(scale_lo, scale_hi).item())\n",
    "        x = x * factor\n",
    "    return x\n",
    "\n",
    "\n",
    "# ========== 数据加载 ==========\n",
    "def load_dataset(name):\n",
    "    data = np.load(os.path.join(data_dir, name))\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    print(f\"Loaded {name}: {X.shape}\")\n",
    "    return torch.FloatTensor(X).unsqueeze(1), torch.LongTensor(y - 1)\n",
    "\n",
    "X_train, y_train = load_dataset(\"train_5subj.npz\")\n",
    "X_val, y_val = load_dataset(\"val_id_5subj.npz\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "# ========== 训练函数 ==========\n",
    "def train_eegnet(model, train_loader, val_loader, epochs=300, lr=3e-4, patience=40):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=12, factor=0.7, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    best_val_acc, best_val_loss = 0.0, float(\"inf\")\n",
    "    patience_cnt = 0\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    best_path = os.path.join(model_dir, \"pooled_eegnet_best.pth\")\n",
    "\n",
    "    print(\"\\n Train HybridNorm EEGNet\")\n",
    "    print(\"Epoch | TrainLoss | ValLoss | TrainAcc | ValAcc | LR\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = augment_batch(data)  #  轻量增强\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_total += target.size(0)\n",
    "            tr_correct += (out.argmax(1) == target).sum().item()\n",
    "\n",
    "        tr_loss /= len(train_loader)\n",
    "        tr_acc = 100. * tr_correct / tr_total\n",
    "\n",
    "        model.eval()\n",
    "        va_loss, va_correct, va_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                out = model(data)\n",
    "                va_loss += criterion(out, target).item()\n",
    "                va_total += target.size(0)\n",
    "                va_correct += (out.argmax(1) == target).sum().item()\n",
    "        va_loss /= len(val_loader)\n",
    "        va_acc = 100. * va_correct / va_total\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        print(f\"{epoch+1:03d} | {tr_loss:.4f} | {va_loss:.4f} | {tr_acc:.2f}% | {va_acc:.2f}% | {lr_now:.1e}\")\n",
    "\n",
    "        # 保存最佳\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc, best_val_loss = va_acc, va_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(f\" Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n Done. Best Val Acc = {best_val_acc:.2f}%  (saved: {best_path})\")\n",
    "    return best_val_acc\n",
    "\n",
    "# ========== 主流程 ==========\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = EEGNetHybridNorm(dropout_rate=0.35)     \n",
    "    best_val = train_eegnet(model, train_loader, val_loader, epochs=300, lr=3e-4, patience=40)\n",
    "\n",
    "    # 载入最佳权重，后续照旧做 features / probs 提取\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, \"pooled_eegnet_best.pth\"), map_location=\"cpu\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783e7ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST_ID ===\n",
      "Accuracy: 55.00%\n",
      "Confusion matrix:\n",
      " [[30 15  3  7]\n",
      " [ 9 37  5  4]\n",
      " [ 9  6 18 22]\n",
      " [ 4  6  9 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.577     0.545     0.561        55\n",
      "           1      0.578     0.673     0.622        55\n",
      "           2      0.514     0.327     0.400        55\n",
      "           3      0.522     0.655     0.581        55\n",
      "\n",
      "    accuracy                          0.550       220\n",
      "   macro avg      0.548     0.550     0.541       220\n",
      "weighted avg      0.548     0.550     0.541       220\n",
      "\n",
      "\n",
      "=== TEST_OOD ===\n",
      "Accuracy: 57.64%\n",
      "Confusion matrix:\n",
      " [[ 61  26  46  11]\n",
      " [  1  97  31  15]\n",
      " [ 10  22  69  43]\n",
      " [  2   5  32 105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.824     0.424     0.560       144\n",
      "           1      0.647     0.674     0.660       144\n",
      "           2      0.388     0.479     0.429       144\n",
      "           3      0.603     0.729     0.660       144\n",
      "\n",
      "    accuracy                          0.576       576\n",
      "   macro avg      0.616     0.576     0.577       576\n",
      "weighted avg      0.616     0.576     0.577       576\n",
      "\n",
      "\n",
      "Summary -> TEST_ID: 55.00%, TEST_OOD: 57.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def eval_split(model, loader, name=\"split\"):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    all_y, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(1).cpu().numpy()\n",
    "            all_pred.append(pred); all_y.append(y.numpy())\n",
    "    y = np.concatenate(all_y); y_pred = np.concatenate(all_pred)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y, y_pred))\n",
    "    print(classification_report(y, y_pred, digits=3))\n",
    "    return acc\n",
    "\n",
    "# 载入最优权重后评估：\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"pooled_eegnet_best.pth\"), map_location=\"cpu\"))\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "acc_id  = eval_split(model, test_id_loader,  name=\"TEST_ID\")\n",
    "acc_ood = eval_split(model, test_ood_loader, name=\"TEST_OOD\")\n",
    "print(f\"\\nSummary -> TEST_ID: {acc_id*100:.2f}%, TEST_OOD: {acc_ood*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
